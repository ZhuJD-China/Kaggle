{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TPOT tutorial on the Titanic dataset\n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:23.915798Z",
     "start_time": "2020-11-22T12:57:22.773928Z"
    }
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:23.949014Z",
     "start_time": "2020-11-22T12:57:23.918996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:23.957455Z",
     "start_time": "2020-11-22T12:57:23.950823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex     Survived\n",
       "female  1           233\n",
       "        0            81\n",
       "male    0           468\n",
       "        1           109\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('Sex').Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:23.966415Z",
     "start_time": "2020-11-22T12:57:23.958823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass  Sex     Survived\n",
       "1       female  1            91\n",
       "                0             3\n",
       "        male    0            77\n",
       "                1            45\n",
       "2       female  1            70\n",
       "                0             6\n",
       "        male    0            91\n",
       "                1            17\n",
       "3       female  0            72\n",
       "                1            72\n",
       "        male    0           300\n",
       "                1            47\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby(['Pclass', 'Sex']).Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:23.988124Z",
     "start_time": "2020-11-22T12:57:23.967854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>female</th>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.968085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.631148</td>\n",
       "      <td>0.368852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>female</th>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.842593</td>\n",
       "      <td>0.157407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>female</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.864553</td>\n",
       "      <td>0.135447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Survived            0.0       1.0\n",
       "Pclass Sex                       \n",
       "1      female  0.031915  0.968085\n",
       "       male    0.631148  0.368852\n",
       "2      female  0.078947  0.921053\n",
       "       male    0.842593  0.157407\n",
       "3      female  0.500000  0.500000\n",
       "       male    0.864553  0.135447"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = pd.crosstab([train_data.Pclass, train_data.Sex], train_data.Survived.astype(float)) # Crosstab 交叉列表取值\n",
    "id.div(id.sum(1).astype(float), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging\n",
    "The first and most important step in using TPOT on any data set is to rename the class/response variable to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:23.993908Z",
     "start_time": "2020-11-22T12:57:23.990653Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.rename(columns={'Survived': 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T05:08:22.467726Z",
     "start_time": "2020-11-22T05:08:22.456814Z"
    }
   },
   "source": [
    "At present, TPOT requires all the data to be in numerical format As we can see below, our data set has 5 categorical variables which contain non-numerical values:Name, Sex, Ticket, Cabin and Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.000135Z",
     "start_time": "2020-11-22T12:57:23.995851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "class            int64\n",
       "Pclass           int64\n",
       "Name            object\n",
       "Sex             object\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Ticket          object\n",
       "Fare           float64\n",
       "Cabin           object\n",
       "Embarked        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the number of levels that each of the five categorical variables have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.006545Z",
     "start_time": "2020-11-22T12:57:24.001483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of levels in category 'Name': \b 891.00 \n",
      "Number of levels in category 'Sex': \b 2.00 \n",
      "Number of levels in category 'Ticket': \b 681.00 \n",
      "Number of levels in category 'Cabin': \b 148.00 \n",
      "Number of levels in category 'Embarked': \b 4.00 \n"
     ]
    }
   ],
   "source": [
    "for cat in ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']:\n",
    "    print(\"Number of levels in category '{0}': \\b {1:2.2f} \".format(cat, train_data[cat].unique().size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.011354Z",
     "start_time": "2020-11-22T12:57:24.007991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levels for catgeory 'Sex': ['male' 'female']\n",
      "Levels for catgeory 'Embarked': ['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "for cat in ['Sex', 'Embarked']:\n",
    "    print(\"Levels for catgeory '{0}': {1}\".format(cat, train_data[cat].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.017530Z",
     "start_time": "2020-11-22T12:57:24.012634Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['Sex'] = train_data['Sex'].map({'male':0,'female':1})\n",
    "train_data['Embarked'] = train_data['Embarked'].map({'S':0,'C':1,'Q':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then code these levels manually into numerical values. For nan i.e. the missing values, we simply replace them with a placeholder value (-999). In fact, we perform this replacement for the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.025912Z",
     "start_time": "2020-11-22T12:57:24.018686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    False\n",
       "class          False\n",
       "Pclass         False\n",
       "Name           False\n",
       "Sex            False\n",
       "Age            False\n",
       "SibSp          False\n",
       "Parch          False\n",
       "Ticket         False\n",
       "Fare           False\n",
       "Cabin          False\n",
       "Embarked       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.fillna(-999)\n",
    "pd.isnull(train_data).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Name and Ticket have so many levels, we drop them from our analysis for the sake of simplicity. For Cabin, we encode the levels as digits using Scikit-learn's MultiLabelBinarizer and treat them as new features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> from sklearn.preprocessing import MultiLabelBinarizer\n",
    ">>> mlb = MultiLabelBinarizer()\n",
    ">>> mlb.fit_transform([(1, 2), (3,)])\n",
    "array([[1, 1, 0],\n",
    "       [0, 0, 1]])\n",
    ">>> mlb.classes_\n",
    "array([1, 2, 3])\n",
    "#################\n",
    ">>> mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}])\n",
    "array([[0, 1, 1],\n",
    "       [1, 0, 0]])\n",
    ">>> list(mlb.classes_)\n",
    "['comedy', 'sci-fi', 'thriller']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.033866Z",
     "start_time": "2020-11-22T12:57:24.027104Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "CabinTrans = mlb.fit_transform([{str(val)} for val in train_data['Cabin'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.038378Z",
     "start_time": "2020-11-22T12:57:24.034972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CabinTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.042984Z",
     "start_time": "2020-11-22T12:57:24.039567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the unused features from the dataset.\n",
    "train_data_new = train_data.drop(['Name','Ticket','Cabin','class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.049098Z",
     "start_time": "2020-11-22T12:57:24.044409Z"
    }
   },
   "outputs": [],
   "source": [
    "assert (len(train_data['Cabin'].unique()) == len(mlb.classes_)), \"Not Equal\" #check correct encoding done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T05:22:30.775948Z",
     "start_time": "2020-11-22T05:22:30.767710Z"
    }
   },
   "source": [
    "We then add the encoded features to form the final dataset to be used with TPOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.053823Z",
     "start_time": "2020-11-22T12:57:24.050365Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_new = np.hstack((train_data_new.values,CabinTrans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.058269Z",
     "start_time": "2020-11-22T12:57:24.055119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 156)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.063105Z",
     "start_time": "2020-11-22T12:57:24.059611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(train_data_new).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping in mind that the final dataset is in the form of a numpy array, we can check the number of features in the final dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.067679Z",
     "start_time": "2020-11-22T12:57:24.064480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_new[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we store the class labels, which we need to predict, in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.072134Z",
     "start_time": "2020-11-22T12:57:24.069727Z"
    }
   },
   "outputs": [],
   "source": [
    "Target_class = train_data['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T05:26:58.211439Z",
     "start_time": "2020-11-22T05:26:58.199114Z"
    }
   },
   "source": [
    "## Data Analysis using TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin our analysis, we need to divide our training data into training and validation sets. The validation set is just to give us an idea of the test set error. The model selection and tuning is entirely taken care of by TPOT, so if we want to, we can skip creating this validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T12:57:24.079149Z",
     "start_time": "2020-11-22T12:57:24.073514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 223)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_indices, validation_indices = training_indices, testing_indices = train_test_split(train_data.index, stratify = Target_class, train_size=0.75, test_size=0.25)\n",
    "training_indices.size, validation_indices.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we proceed to calling the `fit`, `score` and `export` functions on our training dataset. To get a better idea of how these functions work, refer the TPOT documentation [here](http://epistasislab.github.io/tpot/api/).\n",
    "\n",
    "An important TPOT parameter to set is the number of generations. Since our aim is to just illustrate the use of TPOT, we have set maximum optimization time to 2 minutes (max_time_mins=2). On a standard laptop with 4GB RAM, it roughly takes 5 minutes per generation to run. For each added generation, it should take 5 mins more. Thus, for the default value of 100, total run time could be roughly around 8 hours.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=1680.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8308719560094266\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8308719560094266\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8308719560094266\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8308719560094266\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.8383570867467174\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.8428122545168893\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.8428122545168893\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.8428122545168893\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.8428234766019527\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.8428234766019527\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.8443496801705755\n",
      "\n",
      "Best pipeline: RandomForestClassifier(StandardScaler(GaussianNB(StandardScaler(input_matrix))), bootstrap=False, criterion=entropy, max_features=0.5, min_samples_leaf=1, min_samples_split=19, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "               disable_update_check=False, early_stop=None, generations=20,\n",
       "               log_file=None, max_eval_time_mins=5, max_time_mins=None,\n",
       "               memory=None, mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "               periodic_checkpoint_folder=None, population_size=80,\n",
       "               random_state=None, scoring=None, subsample=1.0, template=None,\n",
       "               use_dask=False, verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=20, population_size=80, verbosity=2)\n",
    "tpot.fit(train_data_new[training_indices], Target_class[training_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8026905829596412"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.score(train_data_new[validation_indices], train_data.loc[validation_indices, 'class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.837Z"
    }
   },
   "outputs": [],
   "source": [
    "tpot.export('tpot_titanic_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tpot_titanic_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=None)\n",
    "\n",
    "# Average CV score on the training set was: 0.8443496801705755\n",
    "exported_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    StackingEstimator(estimator=GaussianNB()),\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.5, min_samples_leaf=1, min_samples_split=19, n_estimators=100)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the generated code. As we can see, the random forest classifier performed the best on the given dataset out of all the other models that TPOT currently evaluates on. If we ran TPOT for more generations, then the score should improve further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on the submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>417.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>2.265550</td>\n",
       "      <td>30.272590</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>35.627188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>120.810458</td>\n",
       "      <td>0.841838</td>\n",
       "      <td>14.181209</td>\n",
       "      <td>0.896760</td>\n",
       "      <td>0.981429</td>\n",
       "      <td>55.907576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>892.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>996.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1204.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
       "count   418.000000  418.000000  332.000000  418.000000  418.000000  417.000000\n",
       "mean   1100.500000    2.265550   30.272590    0.447368    0.392344   35.627188\n",
       "std     120.810458    0.841838   14.181209    0.896760    0.981429   55.907576\n",
       "min     892.000000    1.000000    0.170000    0.000000    0.000000    0.000000\n",
       "25%     996.250000    1.000000   21.000000    0.000000    0.000000    7.895800\n",
       "50%    1100.500000    3.000000   27.000000    0.000000    0.000000   14.454200\n",
       "75%    1204.750000    3.000000   39.000000    1.000000    0.000000   31.500000\n",
       "max    1309.000000    3.000000   76.000000    8.000000    9.000000  512.329200"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the submission dataset\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.872Z"
    }
   },
   "outputs": [],
   "source": [
    "for var in ['Cabin']:    #,'Name','Ticket']:\n",
    "    new = list(set(test_data[var]) - set(train_data[var]))\n",
    "    test_data.loc[test_data[var].isin(new), var] = -999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then carry out the data munging steps as done earlier for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.874Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data['Sex'] = test_data['Sex'].map({'male':0,'female':1})\n",
    "test_data['Embarked'] = test_data['Embarked'].map({'S':0,'C':1,'Q':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    False\n",
       "Pclass         False\n",
       "Name           False\n",
       "Sex            False\n",
       "Age            False\n",
       "SibSp          False\n",
       "Parch          False\n",
       "Ticket         False\n",
       "Fare           False\n",
       "Cabin          False\n",
       "Embarked       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.fillna(-999)\n",
    "pd.isnull(test_data).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T05:48:21.560262Z",
     "start_time": "2020-11-22T05:48:21.514140Z"
    }
   },
   "source": [
    "While calling MultiLabelBinarizer for the submission data set, we first fit on the training set again to learn the levels and then transform the submission dataset values. This further ensures that only those levels that were present in the training dataset are transformed. If new levels are still found in the submission dataset then it will return an error and we need to go back and check our earlier step of replacing new levels with the placeholder value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.879Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "SubCabinTrans = mlb.fit([{str(val)} for val in train_data['Cabin'].values]).transform([{str(val)} for val in test_data['Cabin'].values])\n",
    "test_data = test_data.drop(['Name','Ticket','Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Form the new submission data set\n",
    "test_data_new = np.hstack((test_data.values,SubCabinTrans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.883Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([892.    ,   3.    ,   0.    ,  34.5   ,   0.    ,   0.    ,\n",
       "         7.8292,   2.    ,   1.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(test_data_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure equal number of features in both the final training and submission dataset\n",
    "assert (train_data_new.shape[1] == test_data_new.shape[1]), \"Not Equal\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the predictions\n",
    "submission = tpot.predict(test_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the submission file\n",
    "final = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': submission})\n",
    "final.to_csv('TPOT_RES.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-22T12:57:22.894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 2)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EpistasisLab-tpot-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_new:  (891, 156)\n",
      "test_data_new:  (418, 156)\n",
      "Target_class:  (891,)\n",
      "features:  (891, 156)\n"
     ]
    }
   ],
   "source": [
    "print('train_data_new: ', train_data_new.shape)\n",
    "print('test_data_new: ', test_data_new.shape)\n",
    "print('Target_class: ', Target_class.shape)\n",
    "print('features: ', features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8026905829596412\n",
      "0.9326599326599326\n",
      "0.9461883408071748\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "# features = tpot_data.drop('target', axis=1)\n",
    "\n",
    "features = train_data_new\n",
    "training_features, testing_features, training_classes, testing_classes = \\\n",
    "            train_test_split(features, Target_class, random_state=None)\n",
    "\n",
    "exported_pipeline1 = RandomForestClassifier(bootstrap=False, max_features=0.4,\n",
    "                                            min_samples_leaf=1, min_samples_split=9)\n",
    "exported_pipeline1.fit(training_features, training_classes)\n",
    "results1 = exported_pipeline1.predict(test_data_new)\n",
    "\n",
    "print(exported_pipeline1.score(testing_features, testing_classes))\n",
    "print(exported_pipeline1.score(train_data_new, Target_class))\n",
    "print(exported_pipeline1.score(train_data_new[validation_indices], \n",
    "      train_data.loc[validation_indices, 'class'].values))\n",
    "\n",
    "# Create the submission file\n",
    "final = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': results1})\n",
    "final.to_csv('TPOT_RES1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My TPOT-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5336322869955157\n",
      "0.9169472502805837\n",
      "0.9192825112107623\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, Target_class, random_state=None)\n",
    "\n",
    "# Average CV score on the training set was: 0.8443496801705755\n",
    "exported_pipeline2 = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    StackingEstimator(estimator=GaussianNB()),\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.5, \n",
    "                           min_samples_leaf=1, min_samples_split=20, n_estimators=90)\n",
    ")\n",
    "\n",
    "exported_pipeline2.fit(training_features, training_target)\n",
    "results2 = exported_pipeline2.predict(test_data_new)\n",
    "\n",
    "print(exported_pipeline2.score(testing_features, testing_classes))\n",
    "print(exported_pipeline2.score(train_data_new, Target_class))\n",
    "print(exported_pipeline2.score(train_data_new[validation_indices], \n",
    "      train_data.loc[validation_indices, 'class'].values))\n",
    "\n",
    "# Create the submission file\n",
    "final = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': results2})\n",
    "final.to_csv('TPOT_RES2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
